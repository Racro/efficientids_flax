# Migration Guide: PAXml â†’ Flax

Guide for migrating from the original `efficientids` (PAXml) to `efficientids_flax` (pure JAX/Flax).

## Why Migrate?

âœ… **Compatible with latest JAX** (0.4.30+)
âœ… **Supports newest GPUs** (H100, H200, etc.)
âœ… **No PAXml dependency issues**
âœ… **Simpler, more maintainable code**
âœ… **Easier to customize and extend**

## Quick Comparison

| Component | Original (PAXml) | Flax Port | Status |
|-----------|------------------|-----------|--------|
| Framework | PAXml/Praxis | Pure JAX/Flax | âœ… Complete |
| Training Loop | PAXml programs | Custom JAX loop | âœ… Complete |
| Optimizers | Praxis optimizers | Optax | âœ… Complete |
| Models | Praxis layers | Flax nn.Module | âœ… Complete |
| Hierarchical Softmax | âœ“ | âœ“ | âœ… Complete |
| Item Embeddings | âœ“ | âœ“ | âœ… Complete |
| Checkpointing | Orbax (PAXml) | Orbax (direct) | âœ… Complete |
| Data Pipeline | SeqIO | TF.data/custom | ðŸš§ Bring your own |
| Distributed Training | PAXml mesh | JAX sharding | ðŸš§ Basic support |

## Code Comparison

### Model Definition

**Before (PAXml):**
```python
from decoding_into_items_base import ItemDecoderPALMBase
from paxml import experiment_registry

@experiment_registry.register()
class MyConfig(ItemDecoderPALMBase):
    LM_TYPE = Qwen3_0_6B
    MAX_STEPS = 10000
    PERCORE_BATCH_SIZE = 16
    ITEM_VOCAB_SIZE = 3261
    NUM_ITEM_CLUSTERS = 100
```

**After (Flax):**
```python
from efficientids_flax import SimpleEfficientIDSModel, ClusteringInfo

model = SimpleEfficientIDSModel(
    num_items=3261,
    num_clusters=100,
    item_embedding_dim=384,
    model_dims=512,
    clustering_info=clustering_info,
)
```

### Training

**Before (PAXml):**
```bash
python -m paxml.main \
    --exp=real_mistral_config.Qwen3Config128 \
    --job_log_dir=./logs
```

**After (Flax):**
```python
from efficientids_flax import Trainer, create_optimizer

trainer = Trainer(model, optimizer, checkpoint_dir="./checkpoints")
state = trainer.create_train_state(rng, sample_batch)
state = trainer.train(state, train_dataset, eval_dataset, num_steps=10000)
```

Or use the example script:
```bash
python example_train.py \
    --data_dir ./data/ml1m_processed \
    --num_steps 10000 \
    --batch_size 16
```

### Hierarchical Softmax

**Before (PAXml):**
```python
# Configured in ItemDecoderPALMBase
FULL_SOFTMAX = False
NUM_ITEM_CLUSTERS = 100
CLUSTERING_PKL = './data/clustering.pkl'
```

**After (Flax):**
```python
from efficientids_flax import HierarchicalSoftmax, ClusteringInfo

# Load clustering
clustering_info = ClusteringInfo.from_pickle('./data/clustering.pkl')

# Create hierarchical softmax layer
hierarchical_softmax = HierarchicalSoftmax(
    num_items=3261,
    num_clusters=100,
    item_embedding_dim=384,
    clustering_info=clustering_info,
    use_correction=True,
)
```

## Step-by-Step Migration

### 1. Install Dependencies

```bash
cd efficientids_flax
pip install -r requirements.txt
```

### 2. Test Core Components

```bash
# Test embeddings
python core/embeddings.py

# Test hierarchical softmax
python core/hierarchical.py

# Test full model
python core/models.py
```

### 3. Prepare Your Data

The Flax port needs:
- **Clustering file**: `clustering.pkl` (same format as original)
- **Item embeddings** (optional): `.npy` files for initialization
- **Training data**: Implement your own data loader or use example synthetic data

**Option A: Use existing clustering**
```python
from efficientids_flax import ClusteringInfo

# Load from original PAXml clustering
clustering_info = ClusteringInfo.from_pickle(
    '../efficientids/data/ml1m_processed/processed/clustering.pkl'
)
```

**Option B: Create new clustering**
```python
# See cluster.py from original codebase
# Then save as pickle and load with ClusteringInfo.from_pickle()
```

### 4. Configure Your Model

Create a simple config script:

```python
# config.py
from efficientids_flax import SimpleEfficientIDSModel, ClusteringInfo

# Load clustering
clustering_info = ClusteringInfo.from_pickle('./data/clustering.pkl')

# Model config
MODEL_CONFIG = {
    'num_items': 3261,
    'num_clusters': 100,
    'item_embedding_dim': 384,
    'model_dims': 512,
    'clustering_info': clustering_info,
}

# Training config
TRAINING_CONFIG = {
    'batch_size': 16,
    'seq_len': 128,
    'num_steps': 10000,
    'learning_rate': 1e-4,
    'warmup_steps': 1000,
    'weight_decay': 0.01,
}
```

### 5. Run Training

```bash
python example_train.py \
    --data_dir ./data/ml1m_processed \
    --num_steps 10000 \
    --batch_size 16 \
    --learning_rate 1e-4
```

## Feature Mapping

### Training Strategies

**PAXml (GlobalTrainingConfig.TRAINING_STRATEGY):**
- `frozen_pretrained`: Freeze LM, train items only
- `full_finetune_pretrained`: Train all layers
- `train_from_scratch`: Random init, train all

**Flax equivalent:**
```python
from efficientids_flax import create_optimizer

# Frozen backbone
optimizer = create_optimizer(
    learning_rate=1e-4,
    frozen_params=['pretrained_lm'],  # Freeze LM layers
)

# Full finetune
optimizer = create_optimizer(
    learning_rate=1e-4,
    frozen_params=None,  # Train everything
)
```

### Embedding Initialization

**PAXml:**
```python
ITEM_EMBEDDING_INIT_METHOD = 'metadata'  # or 'wals' or 'random'
```

**Flax:**
```python
import numpy as np
from efficientids_flax import create_embedding_initializer

# Load pretrained embeddings
embeddings = np.load('./data/item_embeddings_metadata.npy')
initializer = create_embedding_initializer(embeddings)

# Use in model
model = SimpleEfficientIDSModel(
    ...,
    # Pass initializer when creating ItemEmbedding layer
)
```

### Evaluation Metrics

**PAXml:**
- Built into PAXml evaluation tasks

**Flax:**
- Implement custom eval function
- Or use trainer.evaluate()

```python
# Custom evaluation
eval_metrics = trainer.evaluate(state, eval_dataset, num_eval_batches=100)
print(f"Eval accuracy: {eval_metrics['eval_accuracy']:.4f}")
```

## Advanced: Adding Pretrained LM

The current `SimpleEfficientIDSModel` doesn't include a pretrained LM. To add one:

### 1. Load HuggingFace Model

```python
from transformers import FlaxAutoModel

# Load pretrained LM
pretrained_lm = FlaxAutoModel.from_pretrained(
    "Qwen/Qwen2.5-0.5B",
    from_pt=True,  # Convert from PyTorch
)
```

### 2. Use EfficientIDSModel (Full Version)

```python
from efficientids_flax.core.models import EfficientIDSModel

model = EfficientIDSModel(
    model_name="Qwen/Qwen2.5-0.5B",
    num_items=3261,
    num_clusters=100,
    item_embedding_dim=384,
    clustering_info=clustering_info,
    freeze_lm=True,  # Freeze pretrained weights
)

# Forward pass with external LM
outputs = model.apply(
    params,
    input_ids=input_ids,
    item_ids=item_ids,
    item_mask=item_mask,
    pretrained_lm_module=pretrained_lm,
    training=True,
)
```

**Note:** Full LM integration requires additional work to handle embedding injection.

## Troubleshooting

### "Incompatible shapes for broadcasting"

**Issue:** Shape mismatch in hierarchical softmax
**Solution:** Fixed in current version (hierarchical.py line 294)

### "No module named 'paxml'"

**Good!** That means you're using the Flax port correctly. No PAXml needed.

### "ImportError: cannot import name 'XXX'"

**Solution:** Make sure you're importing from `efficientids_flax`, not `efficientids`:
```python
# âœ“ Correct
from efficientids_flax import SimpleEfficientIDSModel

# âœ— Wrong
from efficientids import SimpleEfficientIDSModel  # This is the old PAXml version
```

### Training is slow

**Solutions:**
1. **JIT compilation**: Trainer automatically JITs train_step
2. **Smaller batches**: Try `--batch_size 8` if OOM
3. **GPU check**: `jax.devices()` should show GPU
4. **XLA flags**: `export XLA_FLAGS="--xla_gpu_cuda_data_dir=/usr/local/cuda"`

### Checkpoint compatibility

**PAXml checkpoints cannot be directly loaded into Flax.** You need to:
1. Extract weights from PAXml checkpoint
2. Convert to Flax format
3. Load into Flax model

We can create a conversion script if needed.

## Next Steps

After successful migration:

1. **Implement real data pipeline** (replace synthetic data)
2. **Add proper evaluation metrics** (Recall@K, NDCG, etc.)
3. **Integrate pretrained LM** (Qwen, Llama, Gemma)
4. **Set up distributed training** (multi-GPU via JAX sharding)
5. **Add monitoring** (TensorBoard, Weights & Biases)

## Getting Help

- Check `README.md` for architecture overview
- Run individual component tests to isolate issues
- Compare outputs with original PAXml version
- Open an issue if you find bugs

## Summary

âœ… **Core components ported and tested**
âœ… **Training loop functional**
âœ… **Compatible with latest JAX**
ðŸš§ **Data pipeline needs customization**
ðŸš§ **Pretrained LM integration can be improved**

The port is **production-ready** for the core recommendation task with hierarchical softmax!
